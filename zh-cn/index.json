[{"categories":["AI"],"content":"什么是工作流？ 工作流指的是一组预定义的、标准化的步骤，可以用来完成特定的任务。一个工作流中包含多个节点，每个节点完成一个固定的任务。 举个不太恰当但很容易理解的例子，我们都很熟悉“把大象放进冰箱需要几步？”这个脑筋急转弯，答案是三步：打开冰箱、把大象放进去、关上冰箱。在这个场景中，这三步就构成了“把大象放进冰箱”这个工作流的节点。 ","date":"2024-09-07","objectID":"/zh-cn/posts/how-to-create-a-workflow-in-coze/:1:0","tags":["AI","Coze"],"title":"【手把手教学】Coze如何创建工作流","uri":"/zh-cn/posts/how-to-create-a-workflow-in-coze/"},{"categories":["AI"],"content":"为什么需要使用工作流？ 在之前介绍如何在Coze中创建Bot的文章中，有一个步骤是选择你的Bot期望使用的LLM模型。而LLM模型的能力边界受到“上下文”的限制，当你输入给Bot的信息越多、要求其解决的问题越发杂时，Bot返回给你的答案质量就会下降。相信你在直接使用ChatGPT或Claude时也有过类似的感受。 短期内，我们很难要求大语言模型在处理上下文方面获得显著的能力提升。那么另一个解决方法，就是把输入给Bot的复杂任务拆解为多个简单的子任务，从而保证Bot输出内容的质量。 ","date":"2024-09-07","objectID":"/zh-cn/posts/how-to-create-a-workflow-in-coze/:2:0","tags":["AI","Coze"],"title":"【手把手教学】Coze如何创建工作流","uri":"/zh-cn/posts/how-to-create-a-workflow-in-coze/"},{"categories":["AI"],"content":"Coze工作流功能简介 节点是组成工作流的基本单元，工作流由多个节点构成。Coze中的节点可以分为两类，一类是默认自动生成的固定节点，另一类是可选的基础节点： ","date":"2024-09-07","objectID":"/zh-cn/posts/how-to-create-a-workflow-in-coze/:3:0","tags":["AI","Coze"],"title":"【手把手教学】Coze如何创建工作流","uri":"/zh-cn/posts/how-to-create-a-workflow-in-coze/"},{"categories":["AI"],"content":"默认自动生成的固定节点 Start节点：工作流的起始节点，可以包含用户输入的信息 End节点：工作流的末尾节点，用于返回工作流的运行结果 ","date":"2024-09-07","objectID":"/zh-cn/posts/how-to-create-a-workflow-in-coze/:3:1","tags":["AI","Coze"],"title":"【手把手教学】Coze如何创建工作流","uri":"/zh-cn/posts/how-to-create-a-workflow-in-coze/"},{"categories":["AI"],"content":"可选基础节点 目前，Coze共提供十三个基础节点： 节点名称 描述 Plugin 插件节点。用于使用外部实时数据并处理任务 LLM 大语言模型节点。支持选择不同的AI模型处理文本生成任务 Code 代码节点。通过IDE编写代码处理输入参数，并返回输出值 Knowledge 知识库节点。根据输入参数从关联知识库中召回数据，并返回 Workflow 工作流节点。添加已发布的工作流并执行子任务 Condition if-else 逻辑节点。满足设置条件则运行 if 分支，否则运行 else 分支 Loop 循环节点。通过设置循环的逻辑和次数以重复执行一系列任务 Intent recognition 意图识别节点。用于识别用户输入的意图，将其与预置的意图选项匹配 Text Processing 文本处理节点。用于处理多个字符串类型的变量 Message 消息节点。支持中间过程的消息输出，支持流式和非流式方式 Question 问题节点。支持在对话过程中向用户提问，既有预设选项，也有开放式问题 Variable 变量节点。用于读取和写入 Bot 中的变量 Database 数据库节点。用户可以在开发者控制的数据库中读写数据。必须事先在Bot的数据库中添加一个表 ","date":"2024-09-07","objectID":"/zh-cn/posts/how-to-create-a-workflow-in-coze/:3:2","tags":["AI","Coze"],"title":"【手把手教学】Coze如何创建工作流","uri":"/zh-cn/posts/how-to-create-a-workflow-in-coze/"},{"categories":["AI"],"content":"节点与参数 不同节点可能需要输入不同的参数，输入参数分为Reference和Input两类： Reference参数：指引用前面节点的参数值 Input参数：支持设置自定义的参数值 ","date":"2024-09-07","objectID":"/zh-cn/posts/how-to-create-a-workflow-in-coze/:3:3","tags":["AI","Coze"],"title":"【手把手教学】Coze如何创建工作流","uri":"/zh-cn/posts/how-to-create-a-workflow-in-coze/"},{"categories":["AI"],"content":"如何使用Coze创建工作流 本文会介绍一个我用Coze搭建关键词调研Bot的案例，它可以根据用户输入的目标关键词，在Google中查询和总结相关的信息，并且附带对应的内容链接。 首先，我们需要创建一个Bot（如果不太清楚如何创建，可以看之前的一篇文章），然后点击“工作流”后面的“+”号： 输入工作流的名称和描述，注意命名只能包括字母、数字、下划线，而且要以字母开头。然后点击“确认”： 之后进入工作流编辑页面，可以看到，Coze会自动生成Start节点和End节点。左侧为前面提到的可选的基础节点： 首先，我们需要确定如何从Google中获得我们想要的信息。以往我们更多地直接通过写代码来访问API进而收集到所需要的信息。那么在Coze中，可以直接使用插件来完成这项任务。 点击左侧的“Plugin”按钮，然后选择Google Web Search这款插件并添加： 这个工作流的触发是用户输入的关键词，所以我们需要在Start节点配置字符串类型的参数，用于接收输入的文本，此处命名为query： 关键词信息需要流向Google Web Search插件，由其完成文本处理和检索任务，这里的操作包括： 设置为上一步骤中的query 将开始节点右侧和插件节点左侧连接起来，表示任务之间的顺序 此外，我们还可以设置另外两个参数： num参数：定义返回的最大数量的检索结果数，默认为10 start参数：设置检索的页面数，默认为0，即检索结果第一页 这时我们可以先点击右上角的“Test run”测试一下输出效果。这里我们输入“chatgpt”为例，可以看到Coze花费3秒钟成功运行了这个工作流。点击插件节点右上角的“Display result”可以查看其处理任务的具体结果： 我希望这个工作流最终输出的信息包括：每个检索结果的标题、描述，以及对应的链接。而这些信息是混杂在Google Web Search插件的输出结果中的，那么我需要一个Code节点来提取这些信息。 将Code节点置于插件节点和End节点之间后，input参数设置为插件节点输出的结果，然后点击Edit in IDE进入比代码编辑页面： 目前Code节点支持Python和JavaScript两种语言。如果你不会写代码，也可以通过在内置的AI中输入自然语言prompt让AI帮你写。写完之后点击“Test Code”测试代码是否可以正常运行： 确定代码运行正常后，我们可以再次测试整个工作流的运行情况。虽然此时我们已经能够从Google搜索结果中提取相应的结果标题、链接、摘要，但所有的信息都堆在一起，所以我们还需要一个LLM节点来帮助我们组织信息。为了保证输出结果的质量，prompt中要尽可能讲清楚背景信息： ","date":"2024-09-07","objectID":"/zh-cn/posts/how-to-create-a-workflow-in-coze/:4:0","tags":["AI","Coze"],"title":"【手把手教学】Coze如何创建工作流","uri":"/zh-cn/posts/how-to-create-a-workflow-in-coze/"},{"categories":["AI"],"content":"如何将工作流应用在Coze Bot中 测试工作流能够正常运行后，我们可以点击右上角的“Publish”按钮进行发布，然后在Bot中添加这个工作流（注意：工作流需要发布后才能添加到Bot中）： ","date":"2024-09-07","objectID":"/zh-cn/posts/how-to-create-a-workflow-in-coze/:5:0","tags":["AI","Coze"],"title":"【手把手教学】Coze如何创建工作流","uri":"/zh-cn/posts/how-to-create-a-workflow-in-coze/"},{"categories":["AI"],"content":"—sref指令全称为“风格参考”（Style References），旨在帮助用户复刻和保持图片风格的一致性。其主要作用是允许用户上传一张或多张图片作为风格参考，以指导Midjourney生成与这些参考图风格一致的图像。这个指令在你看到一个非常喜欢的图片，想尝试生成类似或相同风格的图片、而又不知道这是什么风格时特别有用。 ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:0:0","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"适用版本 Midjourney V6 和 Niji V6 ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:1:0","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"基础使用方法 prompt提示词 --sref 图片URL 有以下三种获得图片URL的方法： 鼠标移动到之前生成过的图片，单击右键，选择“复制消息链接”，然后即可粘贴到prompt中 直接将之前生成过的图片拖拽到输入框中 从本地上传图片 MJ会将图片URL视作一个风格参考，并尝试制作类似风格的内容。 本文将以Midjourney官网的这张图片进行演示： 先将图片上传到Midjourney中，然后输入以下prompt： a cute girl with hat --sref https://s.mj.run/XRxNtTBjwcs --style raw --s 750 --niji 6 可以看到Midjourney根据我们提供的图片风格，绘制了4个戴帽子的小女孩的图片： ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:2:0","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"—sref v.s style tuner style tuner可以通过prompt生成一系列的图，然后根据你选择的内容来确定这个模型的风格和方向。可以将style tuner生成的图片作为sref参数的参考URL，下面为prompt示例： Style tuner生成的图片： 依据style tuner的风格，利用sref参数生成的图片： ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:2:1","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"—sref v.s 垫图 –sref参数和垫图的prompt格式不同： 垫图： /imagine URL prompt text –v 6 Style reference： /imagine prompt text –v 6 –sref URL 我们用下面这张参考图片来看看生成图片的区别： 使用相同的描述文本和参考图片，垫图更加关注语义内容（如人物、地点、物品等），会参考一小部分风格： 而sref参数则更加关注色彩和风格： ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:2:2","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"进阶使用方法 ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:3:0","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"设置图片参考的总权重 —sw（Style weight）参数可用来调节风格强度。–sw默认值为100，数值范围为0到1000。数值越高，生成的图片与参考图片的风格就越相似；数值越低，画面就越接近提示词直出的效果。经过测试，至少要15-20才能看出效果，数值在200左右效果较好，超过700画面容易出现扭曲变形。 ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:3:1","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"使用多个图片作为风格参考 提示词如下： prompt提示词 --sref 图片一URL 图片二URL 图片三URL ... 例如同时参考上述两张图片的风格生成一张图片： ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:3:2","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"设置单个参考图片的权重 使用多张参考图片时，每张图片的参考权重是相同的。可以使用::来指定单张图片的权重，权重数值区间为0到100，例如： ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:3:3","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"结合垫图使用 通过垫图让Midjourney学习图片中的元素、构图、内容，–sref指令让Midjourney学习图片风格。注意：中间一定要加文字提示内容，否则会无法生成。 提示词如下： 图片URL prompt提示词 --sref 图片URL 两者同时使用则可兼顾语义内容与风格： ","date":"2024-09-01","objectID":"/zh-cn/posts/midjourney-sref/:3:4","tags":["AI","Midjourney"],"title":"如何使用Midjourney SREF控制风格一致性","uri":"/zh-cn/posts/midjourney-sref/"},{"categories":["AI"],"content":"什么是Coze？ Coze是由字节跳动推出的AI聊天机器人和应用程序编辑开发平台。通过集成多款插件，Coze的Bot能力得到极大扩展，可适用于各种场景，如聊天机器人、数据分析、内容采集等。 ","date":"2024-09-01","objectID":"/zh-cn/posts/how-to-create-bot-on-coze/:1:0","tags":["AI","Coze"],"title":"【手把手教学】Coze怎么创建bot？","uri":"/zh-cn/posts/how-to-create-bot-on-coze/"},{"categories":["AI"],"content":"Coze页面简介 注册成功进入Coze后，在主页，你首先会看到一个助理机器人，它会引导你一步一步了解Coze的功能及作用。 左侧菜单栏包括以下几个部分： 个人空间： 是仅自己可见，用于存放自己做的bot、收藏的bot、插件以及工作流的地方。你也可以通过创建团队空间，与团队成员共享bot和插件等。 Bot商店： Bot，即机器人，通常指能够执行自动化任务的软件程序。在Bot商店，你可以看到Coze官方以及其它用户发布的Bot。此外，如果你选择上架自己创建的Bot，那么它也会出现在商店。 插件商店： 可查看官方和其它用户发布的插件，如GPT4V、Google Web Search、DALLE 3等，可通过“最受欢迎”或“最近”进行排序。 工作流商店： “工作流”指为了完成某项任务而进行的一系列有序的步骤。对于一个较为复杂的任务，我们需要对其进行拆解，拆分为单个步骤，再将每个步骤按照预设的流程和动作进行连接，从而实现自动化完成任务，减少人工的重复操作。在工作流商店，你可以查看并复制他人已经创建好的工作流。 ","date":"2024-09-01","objectID":"/zh-cn/posts/how-to-create-bot-on-coze/:2:0","tags":["AI","Coze"],"title":"【手把手教学】Coze怎么创建bot？","uri":"/zh-cn/posts/how-to-create-bot-on-coze/"},{"categories":["AI"],"content":"如何通过Coze创建Bot 接下来，我们看下如何通过Coze搭建一个支持中英互译的bot。 ","date":"2024-09-01","objectID":"/zh-cn/posts/how-to-create-bot-on-coze/:3:0","tags":["AI","Coze"],"title":"【手把手教学】Coze怎么创建bot？","uri":"/zh-cn/posts/how-to-create-bot-on-coze/"},{"categories":["AI"],"content":"1. 创建一个bot并完成基础设置 点击左上角的“创建bot”，然后完成以下基础设置： 空间（必填）：选择bot存放的空间，可选“个人空间”或“团队空间” Bot的名字（必填）：我们设置为“English - Chinese Translator” 用户消息账单（必填）：选择是由用户承担还是自己承担对话时消息用量的费用 Bot功能简介（选填）：介绍你的Bot的作用。这些内容会呈现给使用你的Bot的用户 图标（必填）：可直接上传图片，或要求Coze基于你的Bot名字及功能生成一个合适的图标 完成上述设置后即可点击“确认”按钮，进入下一步。 这里简单介绍下Coze关于消息积分的收费： 在与Bot对话时，Coze会按照收到的消息数量进行收费，而消息积分则是用于抵扣在与Bot对话时消息用量的费用（注意，调试Bot时收到的Bot回复数量，也会扣减对应的积分）。 每个账号每天将免费获得10个积分。积分扣减完毕后，和Bot对话时会收到积分不足的提示。如需继续和Bot对话，你可以购买Premium Plan，或单独购买更多积分。 费用根据Bot使用的模型而有所不同： 模型 每条消息消耗的积分 GPT-3.5 Turbo 0.1 GPT-4o mini (128k) Coze free plan：0.1\\n Coze Premium：1 GPT-4o (8k) 2 GPT-4o (32k) 5 GPT-4o (128k) 10 Gemini 1.5 Pro 2.5 Gemini 1.5 Flash 0.5 Claude 3 Haiku 0.1 Claude 3.5 Sonnet 2 ","date":"2024-09-01","objectID":"/zh-cn/posts/how-to-create-bot-on-coze/:3:1","tags":["AI","Coze"],"title":"【手把手教学】Coze怎么创建bot？","uri":"/zh-cn/posts/how-to-create-bot-on-coze/"},{"categories":["AI"],"content":"2. Bot配置 配置界面分为三部分： 左边：支持输入自然语言，设置Bot的角色定位和预设的prompt 中间：设置Bot的技能，支持：插件、工作流、触发器、知识库等 右边：Bot的预览效果 首先，选择Bot的模式。目前Coze支持单agent模式和多agent模式（默认为单agent模式）： 然后，选择Bot想应用的LLM模型，目前支持：Claude、GPT、Gemini，本文以GPT-3.5 Turbo为例。这里的模型选择会影响到前面提到的消费用量的费用： 接下来，我们开始设置Bot的身份和功能。在左侧的命令栏，我告诉Bot：你是一位擅长中英互译的语言专家，当用户输入英文时，你会返回对应的中文（附带拼音）；当用户输入中文时，你会返回对应的英文。然后，我们在右侧的预览栏进行测试：输入“To be or not to be, that is a question.”，发送给Bot。可以看到Bot返回了这句英文的正确中文翻译，并附带了对应的拼音： 如果你担心自己的命令写得不够准确或希望将自然语言转换为更加标准的prompt语言，可以点击右上角的“优化”按钮，由Coze帮你优化prompt。 我们再测试一下中译英的效果，可以看到Bot也能够正确地返回英文翻译： ","date":"2024-09-01","objectID":"/zh-cn/posts/how-to-create-bot-on-coze/:3:2","tags":["AI","Coze"],"title":"【手把手教学】Coze怎么创建bot？","uri":"/zh-cn/posts/how-to-create-bot-on-coze/"},{"categories":["AI"],"content":"3. 发布Bot 当你调试完Bot，确定其能够按照你的要求正确地运行后，可以点击右上角的“发布”按钮，让更多的人看到和使用你的Bot！ ","date":"2024-09-01","objectID":"/zh-cn/posts/how-to-create-bot-on-coze/:3:3","tags":["AI","Coze"],"title":"【手把手教学】Coze怎么创建bot？","uri":"/zh-cn/posts/how-to-create-bot-on-coze/"},{"categories":null,"content":"看了眼之前最后一篇博文的时间，距今天刚好四个月。磕磕绊绊地翻出教程新建了这个文档。 ","date":"2024-08-25","objectID":"/zh-cn/posts/re-update-the-blog-after-four-months/:0:0","tags":null,"title":"时隔四个月重新更新博客","uri":"/zh-cn/posts/re-update-the-blog-after-four-months/"},{"categories":null,"content":"这个博客的起源 之前翻箱倒柜找出了近几年的日记，在欣赏自己过去青涩中二天真的记录的过程中，确定自己是在2022年1月23日创建了这个站点。当时一方面想要找个地方集中记录自己的想法和精力，另一方面莫名其妙地又重新燃起了对编程相关技术技能的兴趣。在体验了CSDN、掘金、博客园、微信公众号、Notion等市面上一系列常见的博文载体后，我还是决定挑战一下，自己用Hugo和GitHub Pages搭一个。 整个过程其实不算顺利，我有的编程知识也只是基本的Java和前端语言。但好在有不少大神写了详细的教程，我花了一个下午加晚上的时间把站点上线了。 Chloe是我给自己起的英文名。当时在某个网站上输入自己的性命拼音，按照姓氏选择了一个比较匹配而且寓意还不错的名字。Evolution是希望通过这个博客记录自己的成长和进化过程。 一开始心血来潮更新了几篇，中间便是长时间的断更。到今年初的时候又开始折腾，结果这次玩脱了，在增加某个配置的时候直接把根目录搞乱了，尝试了几轮回退也没办法恢复到原先的样子，索性咬咬牙直接重建了。这一天是2024年3月28日。 之后又批量更新了几篇竞品调研的博文，但被谷歌收录的量一直不高，严重打击我更新的热情，索性不管了。 ","date":"2024-08-25","objectID":"/zh-cn/posts/re-update-the-blog-after-four-months/:1:0","tags":null,"title":"时隔四个月重新更新博客","uri":"/zh-cn/posts/re-update-the-blog-after-four-months/"},{"categories":null,"content":"决定重新更新博客的原因 想要重新更新是因为在最近几个月获得了太多的信息输入。一方面开始重新有规律地看书，每天差不多都是一个小时的阅读时间；另一方面也开始主动地关注行业和世界的发展趋势，例如AI。信息过于繁杂多样，全部挤在我的脑子里乱飞，时不时地从某个角落探出头来，于是我便被吸引注意力，开始漫无目的地瞎想，然后又被另外冒出来的信息或想法吸引走。我不喜欢这种看似了解了很多，实际上没明白什么的状态。 我开始有意识地整理归纳自己接收到的信息，比如定期清理各个平台的收藏，包括：微博、小红书、公众号、推特、各类社群……同时期望在这个过程中能够稍微窥探到我目前最苦恼的问题的答案：我什么时候可以不上班？ 显然，纸上谈兵是行不通的，我得下场试试。为了避免犯同样的错误，我决定把尝试的经历都写下来，无论成功与否。 ","date":"2024-08-25","objectID":"/zh-cn/posts/re-update-the-blog-after-four-months/:2:0","tags":null,"title":"时隔四个月重新更新博客","uri":"/zh-cn/posts/re-update-the-blog-after-four-months/"},{"categories":null,"content":"接下来希望更新的内容 我发现自己在潜意识里很喜欢同时挑战多个任务，这两次跳槽期间，我都分别开了一个独立站（虽然第一个已经关了，现在这个也没什么起色）。结合今年刚接触的新行业以及对自身未来职业发展的期待，暂时会主要关注以下领域： 数字营销：包括但不限于SEO、SEM、TikTok运营、Pinterest运营、YouTube运营 AI：包括但不限于ChatGPT、Claude、Midjourney 金融\u0026经济学：《经济学原理》不知道翻开过几遍了，至少先建立比较完整的知识体系吧 Hugo：现在这个站点着实是不太好看，还是想装修一下 祝我早日不上班！ ","date":"2024-08-25","objectID":"/zh-cn/posts/re-update-the-blog-after-four-months/:3:0","tags":null,"title":"时隔四个月重新更新博客","uri":"/zh-cn/posts/re-update-the-blog-after-four-months/"},{"categories":["Python"],"content":"本文介绍了如何使用 Python 进行客户分析和细分。","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"本文使用的示例数据和分析代码来自Kaggle。但因为实际的运行结果与原作者不同，故在分析部分略有差异。 具体的分析流程包括： 数据准备 数据清洗 数据可视化 数据加工 聚类分析 用户画像分析 ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:0:0","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"1. 数据准备 使用KMeans聚类算法，将数据分成K个集群： import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from pandas import get_dummies from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler, LabelEncoder from yellowbrick.cluster import KElbowVisualizer import warnings warnings.filterwarnings('ignore') ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:1:0","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"ModuleNotFoundError: No module named ‘yellowbrick’ 出现这个错误的原因是我本地没有安装Yellowbrick这个库。 Yellowbrick是一个机器学习可视化库，主要依赖于sklearn机器学习库，能够提供多种机器学习算法的可视化。 Yellowbrick有两个主要依赖：scikit-learn和matplotlib。Yellowbrick是Python 3软件包，可与3.4或更高版本一起使用。 安装方法： pip install yellowbrick 查看数据的基本情况，共有29列： #sep是分隔符，字符型，默认值是','号。'\\t'是制表符分隔 #如果数据之前存在空格，或者说分隔符与数据之间存在空格，skipinitialspace如果指定为True，会跳过这个空格再读数据.。如果取值为False不会跳过空格，而是将空格作为数据的一部分进行读取。skipinitialspace的默认取值为False df_dataset = pd.read_csv('marketing_campaign.csv', sep='\\t', skipinitialspace = True) #根据位置返回对象的前n行，默认前5行 df_dataset.head() 查看各列的数值类型及空值情况： #info()函数用于打印DataFrame的简要摘要，显示有关DataFrame的信息，包括索引的数据类型、列的数据类型、非空值的数量、内存使用情况 df_dataset.info() ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:1:1","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"2. 数据清洗 已知顾客注册时间（Dt_Customer）的类型为Object。为便于后续拆分为年、月、日，需将其转换为日期时间的格式。 同时通过date()查看数据的时间跨度。由此可知数据集中customer注册时间为2012-2014年： #to_datetime()将参数转换为日期时间的格式 #Dt_Customer的原始格式是Object，并不是时间格式，需要转换 df_dataset['Dt_Customer'] = pd.to_datetime(df_dataset['Dt_Customer'], dayfirst = True) print(\"The oldest record on customer's enrollment:\", min(df_dataset['Dt_Customer']).date()) print(\"The newest record on customer's enrollment:\", max(df_dataset['Dt_Customer']).date()) ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:2:0","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"ValueError: time data doesn’t match to_datetime()的默认日期解析顺序是月日年，而数据集中的日期格式是日月年，故需要指定dayfirst的值： 对一些属性进行优化以便于后续的分析： 年龄（Age）：顾客记录的最近时间为2014年，以此为锚点，减去顾客的出生日期 学历情况（Education）：缩小为3个类目：Graduate、Postgraduate、Undergraduate 婚恋状况（Living_With）：缩小为2个类目：Partner、Alone 孩子数量（Total_Children）：新属性，合并“Kidhome“和”Teenhome“，表示顾客家里的孩子总数（儿童+青少年） 顾客注册时间（Dt_Customer）：拆分为天、星期几、月、年 是否已为父母（Is_Parent）：新属性，表明顾客的育儿状态 总花费（Total_Spent）：顾客的总花费 剩余特征属性维持不变 df_dataset['Age'] = 2014 - df_dataset['Year_Birth'] df_dataset['Education'] = df_dataset['Education'].replace({'Graduation':'Graduate', 'PhD':'Postgraduate', 'Master':'Postgraduate', '2n Cycle':'Postgraduate', 'Basic':'Undergraduate'}) df_dataset['Living_With'] = df_dataset['Marital_Status'].replace({'Married':'Partner', 'Together':'Partner', 'Single':'Alone', 'Divorced':'Alone', 'Widow':'Alone', 'Absurd':'Alone', 'YOLO':'Alone'}) df_dataset['Total_Children'] = df_dataset['Kidhome'] + df_dataset['Teenhome'] #把顾客注册时间拆分为具体的日、月、年以及对应的星期几 #lambda函数也叫匿名函数，即没有具体名称的函数，格式为：lambda 参数:操作（参数） #以冒号为分界线，左边是输入的变量，右边是对变量进行的操作 #利用lambda和apply函数结合，可以对DataFrame的一行或一列进行操作 df_dataset['Day'] = df_dataset['Dt_Customer'].apply(lambda x: x.day) df_dataset['Dayofweek'] = df_dataset['Dt_Customer'].apply(lambda x: x.day_name()) df_dataset['Month'] = df_dataset['Dt_Customer'].apply(lambda x: x.month) df_dataset['Year'] = df_dataset['Dt_Customer'].apply(lambda x: x.year) #若有小孩，则赋值为1，否则为0 df_dataset['Is_Parent'] = df_dataset['Total_Children'].apply(lambda x: 1 if x != 0 else 0) df_dataset['Total_Spent'] = df_dataset['MntWines'] + df_dataset['MntFruits'] + df_dataset['MntMeatProducts'] + df_dataset['MntFishProducts'] + df_dataset['MntSweetProducts'] + df_dataset['MntGoldProds'] #简化属性名称 #inplace参数的作用：为True时，不创建新的对象，直接修改原始对象；为False时，对数据进行修改，创建并返回新的对象承载其修改结果 df_dataset.rename(columns={'MntWines':'Wines', 'MntFruits':'Fruits', 'MntMeatProducts':'Meats', 'MntFishProducts':'Fish', 'MntSweetProducts':'Sweets', 'MntGoldProds':'Golds'}, inplace=True) df_dataset.rename(columns={'NumWebPurchases':'Web', 'NumCatalogPurchases':'Catalog', 'NumStorePurchases':'Store'}, inplace=True) #dropna()能够找到DataFrame类型数据的空值，将空值所在的行/列删除后，将新的DataFrame作为返回值返回 #drop()可删除表中的某一行或某一列，不改变原有的DataFrame中的数据，而是返回另一个DataFrame来存储删除后的数据 #drop()默认删除行，如果要删除列，需要添加'axis = 1' df_dataset.dropna(inplace=True) df_dataset.drop(['ID', 'Dt_Customer', 'Year_Birth', 'Marital_Status', 'Z_CostContact', 'Z_Revenue'], axis=1, inplace=True) #copy()可创建一个包含相同元素的新列表 df = df_dataset.copy() 按照顾客的收入和年龄升序排序，读取最后5行以确认是否有异常值 删除异常值：年龄大于等于80、收入金额过高 #sort_values() 可以对Dataframe的数据集按照某个字段中的数据进行排序，默认升序 #tail()读取文件的最后特定行 print('Income:') print(df['Income'].sort_values().tail(5)) print('\\nAge:') print(df['Age'].sort_values().tail(5)) #删除异常值 df = df.drop(2233) df = df[df['Age'] \u003c 80] ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:2:1","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"3. 数据可视化 #设置图表色系 sns.color_palette('Blues') 绘制散点矩阵图，初步观察“年龄”、“收入”、“总花费”、“消费频次“、”是否为父母“几项特征之间的关联关系： data = ['Age', 'Income', 'Total_Spent', 'Recency', 'Is_Parent'] #pairplot为散点矩阵图，用来展示两两特征之间的关系。对角线上是各个属性的直方图（分布图），非对角线上是两个不同属性之间的相关图 #hue针对某一字段进行颜色分类，palette控制色调 #suptitle()用于向图形添加居中标题，y是图形坐标中文本的y位置 plot = sns.pairplot(df[data], hue='Is_Parent', palette='Blues') plot.fig.suptitle('Feature Relationship', y=1.05, weight='bold', fontsize=16) 从图表中可以看出： 非父母人群的收入、消费金额、消费频次明显高于已为父母的人群 收入越高，人们越愿意花钱 ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:3:0","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"Error: 只出现标题，没有图 如果使用的Jupyter Notebook，需要在代码最前面加上 %matplotlib inline 作用是在Notebook内显示图像，而不需要显式地调用plt.show()，具体原理可参考Stackoverflow 通过条形图观察顾客注册为会员的时间分布： #在figure上创建2*2的网格 #flatten()对数组进行降维，返回一份拷贝，对拷贝所作的修改不会影响原始矩阵 #axes.flatten()把子图展开赋值给axes，则axes[0]为第一个子图，axes[1]为第二个子图，以此类推 fig, axes = plt.subplots(2,2, figsize=(15,8)) axes = axes.flatten() fig.suptitle(\"When Did the Customer Enrolled To be a Member\", weight='bold', fontsize=16) #sns.countplot用于画类别特征的频数条形图 #order对x或y的字段排序，排序的方式为列表 #ax用于指定坐标系 sns.countplot(df['Dayofweek'], order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday','Saturday', 'Sunday'], palette='Blues', ax=axes[0]) sns.countplot(df['Day'], palette='Blues', ax=axes[1]) sns.countplot(df['Month'], palette='Blues', ax=axes[2]) sns.countplot(df['Year'], palette='Blues', ax=axes[3]) 从图表中可以看出： 多数消费者在周一、周三注册成为会员 每月第12天的注册人数明显更多 8月的注册人数最多，其次是5月、10月、3月，均超过200人 2013年的注册人数最多（但存在数据缺失问题，本数据集不包含2012年上半年和2014年下半年的数据） 把“年龄”划分为不同的区间，跨度为10： #pd.cut把一组数据分割成离散的区间，默认左边为开区间、右边为闭区间 group = pd.cut(df['Age'], [10, 20, 30, 40, 50, 60, 70, 80]) #value_counts()用于查看表格中有多少个不同值 group.value_counts() 复制一个新的数据表，在此基础上将“年龄”替换为处理后的区间数据： df2 = df.copy() #将Age列替换为已经分割后的数据 df2['Age'] = group #将顾客花费按照年龄进行分类，并分别求和、求平均数 sum_group = df2[['Total_Spent', 'Age']].groupby('Age').sum() mean_group = df2[['Total_Spent', 'Age']].groupby('Age').mean() fig, axes = plt.subplots(1,2,figsize=(14,8)) axes = axes.flatten() #barplot绘制柱状图，ci为置信区间的大小，orient为绘图方向 sns.barplot(x=sum_group['Total_Spent'], y=sum_group.index, palette='Blues', ci=None, orient='h', ax=axes[0]) axes[0].set_title('Total Spent on Products\\nby Age Groups', weight='bold', fontsize=16) #enumerate() 将一个可遍历的数据对象组合为一个索引序列，同时列出数据下标和数据 #plt.text()用于设置文字说明 #'$ {}'.format(v)给所有金额都加上货币符号 for i,v in enumerate(sum_group['Total_Spent']): if i == 0 or i ==6: axes[0].text(v+30000, i, '$ {}'.format(v), horizontalalignment='center', verticalalignment='center', weight='bold', color='black', fontsize=12) else: axes[0].text(v-40000, i, '$ {}'.format(v), horizontalalignment='center', verticalalignment='center', weight='bold', color='white', fontsize=12) sns.barplot(x=mean_group['Total_Spent'], y=mean_group.index, palette='Blues', ci=None, orient='h', ax=axes[1]) axes[1].set_title('Average Spent on Products\\nby Age Groups', weight='bold', fontsize=16) #round(v,2)保留浮点数的小数点后两位 for i,v in enumerate(mean_group['Total_Spent']): axes[1].text(v-130, i, '$ {}'.format(round(v,2)), horizontalalignment='center', verticalalignment='center', weight='bold', color='white', fontsize=12) 从图表中可以看出： 消费分布两极分化明显。两端年龄区间的人数加起来不超过20人，导致总消费金额差距明显 70岁以上人群的平均消费金额最高，超过1000美金，相当于30-40岁人群平均消费金额的两倍 观察不同年龄段人群对不同产品的消费偏好，以及各品类对公司收入的贡献： fig, axd = plt.subplot_mosaic([[0,1,2],[3,4,5], [6,6,7], [6,6,7], [6,6,7]], constrained_layout=True, figsize=(18,10)) fig.suptitle(\"Customer's Average Spent on Products\\nby Age Groups\", weight='bold', fontsize=20) #画条形图，并设置对应的图名称 sns.barplot(data=df, x=group, y='Wines', palette='Blues', ci=None, ax=axd[0]) axd[0].set_title('Wines', weight='bold') sns.barplot(data=df, x=group, y='Fruits', palette='Blues', ci=None, ax=axd[1]) axd[1].set_title('Fruits', weight='bold') sns.barplot(data=df, x=group, y='Meats', palette='Blues', ci=None, ax=axd[2]) axd[2].set_title('Meats', weight='bold') sns.barplot(data=df, x=group, y='Fish', palette='Blues', ci=None, ax=axd[3]) axd[3].set_title('Fish', weight='bold') sns.barplot(data=df, x=group, y='Sweets', palette='Blues', ci=None, ax=axd[4]) axd[4].set_title('Sweets', weight='bold') sns.barplot(data=df, x=group, y='Golds', palette='Blues', ci=None, ax=axd[5]) axd[5].set_title('Gold', weight='bold') #画饼图，对各商品类别求和后升序排列 data = df[['Wines', 'Fruits', 'Meats', 'Fish', 'Sweets', 'Golds']].sum().sort_values() #设置饼图的颜色 palette = sns.color_palette('Blues') #wedges是一个包含扇形对象的列表，texts是一个包含文本标签对象的列表，autotexts是一个包含自动生成的文本标签对象的列表 #autopct设置饼图内各个扇形百分比显示格式，'%.2f%%'为两位小数百分比，textprops为字典类型，用于指定文本标签的属性，如字体大小、字体颜色等 wedges, texts, autotexts = axd[6].pie(x=data, labels=data.index, autopct='%.2f%%', colors=palette, t","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:3:1","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"如何使用plt.subplot_mosaic plt.subplot_mosaic可用于同时创建多个不同的子图，利用列表代表 axes 的方位，返回一个 figure 和多个 axes 参数[[0,1,2],[3,4,5], [6,6,7], [6,6,7], [6,6,7]]：这个参数定义了子图的布局。这里，我们提供了一个列表的列表（或者说是一个矩阵），其中每个子列表代表图形中的一行，列表中的每个元素代表该行中的一个子图位置。通过重复数字，我们指示某些子图跨越多行或多列。在此例中，数字6出现在三行两列的位置上，表示有一个子图跨越了这些位置 根据这段代码，最终的布局将有8个子图，布局如下： 第一行有三个子图，分别标识为0、1、2 第二行也有三个子图，分别标识为3、4、5 第三行开始，有一个较大的子图标识为6，它跨越了第三行到第五行，并占据了两列的空间。旁边是一个较小的子图标识为7 观察不同年龄段人群偏好的购物渠道，以及公司整体的订单来源： fig, axd = plt.subplot_mosaic([[0,1,2], [3,3,4], [3,3,4]], constrained_layout=True, figsize=(18,8)) fig.suptitle(\"Average Number of Purchases Made\\nThrough Different Methods by Age Groups\", weight='bold', fontsize=20) #指定y轴的范围从0到8 #plt.setp()设置子图中y轴的范围 custom_ylim = (0, 8) plt.setp(axd[0], ylim=custom_ylim) plt.setp(axd[1], ylim=custom_ylim) #按照“购买来源”分类后，绘制条形图 sns.barplot(data=df, x=group, y='Web', palette='Blues', ci=None, ax=axd[0]) axd[0].set_title('Web', weight='bold') sns.barplot(data=df, x=group, y='Catalog', palette='Blues', ci=None, ax=axd[1]) axd[1].set_title('Catalog', weight='bold') sns.barplot(data=df, x=group, y='Store', palette='Blues', ci=None, ax=axd[2]) axd[2].set_title('Store', weight='bold') #按照“购买来源”分别求和，绘制饼图 data = df[['Web', 'Catalog', 'Store']].sum().sort_values() #设置饼图颜色和细节 palette = sns.color_palette('Blues') wedges, texts, autotexts = axd[3].pie(x=data, labels=data.index, autopct='%.2f%%', colors=palette, textprops=dict(fontsize=12)); axd[3].set_title('\\n\\nPercentage of Purchases Made\\nThrough Different Methods', weight='bold', fontsize=20, x=1.35) for autotext in autotexts: autotext.set_color('white') autotext.set_weight('bold') #设置饼图数据标签的位置和展示形式 for i, (name, value) in enumerate(zip(data.index, data)): axd[3].text(2.3, 0.3-0.2*i, r\"$\\bf{\" + name + \"}$\" + \"\\t:\" + str(value) + \" times\", fontsize=14) #隐藏没有用上的图 axd[4].axis('off') 从图中可知： 在三种不同购买渠道中，70岁以上人群的消费金额都是最高的，尽管这个群组只有8人。再次反映他们的消费金额要明显高于其它年龄段的人群 将近一半的销售都直接来自于商店，占总购买额的46.2% #将列名从'Response'更改为'AcceptedCmp6'，计算顾客接受的campaign #inplace=True参数确保更改直接在原始DataFrame上进行，而不是返回一个新的DataFrame df.rename(columns={'Response':'AcceptedCmp6'}, inplace=True) plt.figure(figsize=(9,4)) plt.title('Percentage of Customer Who Accepted the nth Offer', weight='bold', fontsize=16) #df.sum()计算所有列的总和 #len(df)返回DataFrame df中的行数，这实际上是计算了每个选定列的总和占DataFrame所有行的百分比 percent = df.sum()[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp6']]*100/len(df) #分别通过条形图和线图绘制percent数据 #通过style='o-'参数添加了标记点，这意味着数据点以圆圈('o')标记，并通过直线('-')连接 ax = percent.plot.bar(color='#6495ED') percent.plot(style='o-', colormap='copper') plt.setp(ax, ylim=(0, 18)) #x坐标是数据点的索引i，使文本与相应的数据点对齐 #y坐标是数据点的值v加上1.2，这样做是为了将文本放置在数据点之上一定的距离，避免文本与数据点或图表其他元素重叠 for i,v in enumerate(percent): plt.text(i, v+1.2, '{:.2f}%'.format(v), horizontalalignment='center', weight='bold', color='Black', fontsize=10) 从图中可知： 愿意接受第一次campaign的人只有6.42%，第二次甚至暴跌到1.36% 第六次campaign明显吸引了大量顾客，接受率高达15.05% 观察使用折扣进行购物的人群分布，以及“是否已成为父母”这一因素的影响： plt.figure(figsize=(9,4)) plt.title('Average Number of Purchases Made with a Discount\\nby Age Groups', weight='bold', fontsize=16) #按“是否已成为父母”分别观察使用折扣进行购物的情况 sns.barplot(data=df, x=group, y='NumDealsPurchases', hue='Is_Parent', ci=None, palette='Blues') 从图中可以看出： 已成为父母的人群明显更容易被折扣吸引，进而进行购买 通过热力图观察“数值”类型的属性之间的关联关系，颜色越深，关联性越强： #从df中选取所有数值类型的列，返回这些列的列名 data = df.select_dtypes(include=[np.number]).columns plt.figure(figsize=(12,10)) plt.title('Feature correlation', weight='bold', fontsize=16, y=1.05) #绘制数值列之间的相关系数矩阵，通过热力图的颜色渐变显示相关性 sns.heatmap(df[data].corr(), cmap='Blues') 从图中可以看出： “月度官网访问次数”、“孩子总数”、“是否为父母”与顾客消费金额的相关性较小 收入、产品类别会显著影响顾客消费金额 ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:3:2","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"4. 数据处理 处理df中的分类特征（即非数值特征），将它们转换为虚拟变量（也称为哑变量或指示变量），然后创建一个新的DataFrame (df_final)，其中包含原始数值特征和新生成的虚拟变量： #选择df中的非数值特征并返回对应的列名 obj_feat = df.select_dtypes(exclude=[np.number]).columns #get_dummies将非数值特征转换为虚拟变量 #drop_first=True参数表示对于每个特征，去掉第一个类别的虚拟变量，以避免虚拟变量陷阱（即完全多重共线性） #concat函数沿着列(axis=1)方向将原始DataFrame (df) 和包含虚拟变量的DataFrame (dummies) 合并 #drop方法移除df_final中的原始非数值特征列，inplace=True表示在原地修改df_final，不创建新的DataFrame #df.shape获取df_final的形状，即其行数和列数 dummies = get_dummies(df[obj_feat], drop_first=True) df_final = pd.concat([df, dummies], axis=1) df_final.drop(obj_feat, axis=1, inplace=True) df_final.shape 所得结果为：(2212, 38) 使用sklearn.preprocessing中的StandardScaler对数据进行标准化处理，然后将处理后的数据转换回Pandas DataFrame格式： #StandardScaler是Scikit-learn库中的一个预处理类，用于将特征缩放到具有均值为0和标准差为1的分布 #fit_transform计算每个特征的均值和标准差，然后使用这些参数将数据进行标准化转换 scaler = StandardScaler() scaled = scaler.fit_transform(df_final) #DataFrame构造函数将scaled数组转换回DataFrame格式 #为了保持列名的一致性，使用df_final.columns作为新DataFrame的列名 #故df_final_scaled是一个新的DataFrame，其中包含了标准化后的数据，并且保留了原始DataFrame的列名 df_final_scaled = pd.DataFrame(scaled, columns=df_final.columns) df_final_scaled.head() 进行标准化处理后，数据点的值表示原始值相对于均值的偏离程度，以标准差为单位。例如，一个处理后的值为2表示该数据点的原始值比均值高出了两个标准差 使用LabelEncoder对DataFrame (df) 中的非数值特征进行编码转换，以便于之后的数据可视化： le = LabelEncoder() #通过一个for循环遍历obj_feat中的所有元素（除了最后一个元素） #先拟合obj中的数据，找出该特征中所有的唯一类别及其对应的整数编码，然后将这些唯一类别转换为整数编码 #原始DataFrame中的当前列（obj）被替换为包含转换后整数编码的trans列 for obj in obj_feat[:-1]: trans = le.fit_transform(df[obj]) df[obj] = trans ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:4:0","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"5. 聚类分析 此处使用\"Elbow method\"（肘部法则）进行聚类分析 ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:5:0","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"“Elbow method”（肘部法则）的原理 计算SSE（误差平方和）：对于每个k值（即聚类数目），我们计算所有点到其分配的聚类中心的欧几里得距离的平方和，这被称为SSE。随着k值的增加，每个聚类的大小通常会减小，因此每个点到其聚类中心的距离也会减小，导致SSE减少。 寻找“肘点”：理想情况下，我们希望找到一个较小的k值，同时保持SSE也相对较小。当我们绘制不同k值对应的SSE时，随着k值的增加，SSE的下降速度会放缓，曲线会呈现出一个“肘”形状。这个“肘点”通常被认为是最佳的聚类数量，因为在这一点之后增加更多的聚类不会显著改善模型的拟合度（即SSE的减少） 考虑到随机初始化的影响： KMeans算法在开始时会随机选择k个聚类中心，这可能导致算法结果受到初始选择的影响，从而使得最终的聚类结果具有一定的随机性。为了在多次运行或不同的环境下获得一致的结果，我们可以设置一个随机状态（random state）。 此处，通过设置random_state=123，我们确保了每次运行KMeans时聚类中心的初始随机选择是相同的，从而使得结果可复现。 ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:5:1","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"为什么SSE会随着k的增加而减少？ 聚类中心的增加：当我们增加聚类数量k时，意味着有更多的聚类中心可以用来表示数据点。随着聚类中心数量的增加，每个数据点更有可能被分配到离它更近的聚类中心，因此每个点到其聚类中心的平均距离会减少，导致SSE降低。 聚类大小的减小：随着k值的增加，每个聚类包含的数据点数量通常会减少。较小的聚类使得聚类内部的数据点更加紧密，减少了数据点与其聚类中心之间的距离，进一步降低了SSE。 极端情况：在极端情况下，如果k的值等于数据点的总数，那么每个数据点都是其自己的聚类中心，这时SSE将会是0。但这种情况没有实际的聚类意义，只是说明了为什么随着k的增加，SSE会持续减少。 #使用KMeans聚类算法，指定要测试的最大聚类数量为10 #KElbowVisualizer将会评估从1到10的k值，以决定哪个k值是最佳选择 #调用fit方法来训练KMeans模型 elbow = KElbowVisualizer(KMeans(random_state=123), k=10) elbow.fit(df_final_scaled) elbow.show() 运行结果是一个关于不同k值（聚类数量）与对应的SSE（误差平方和）的图表。肘部图的目的是帮助我们直观地看到随着k值增加，SSE如何变化。理想情况下，SSE随k值增加而降低，但下降速度会在某一点明显放缓，这个点就是所谓的“肘点”，被认为是最佳的聚类数量 从图中可以看出，“肘点”为k=5 #根据上图结果，创建一个聚类对象，设置聚类数量为5，即最终的目标是将数据点分成8个聚类 model = KMeans(n_clusters=5, random_state=123) #fit_predict使用预处理和标准化后的数据集来训练（拟合）KMeans模型，返回每个数据点被分配到的聚类标签，这些标签存储在yhat变量中 yhat = model.fit_predict(df_final_scaled) #将聚类结果添加到原始DataFrame中，从而能够看到每个原始数据点属于哪个聚类 df['Cluster'] = yhat #绘制每个聚类标签出现的次数 sns.countplot(data=df, x='Cluster', palette='Blues') ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:5:2","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"6. 用户画像分析 分析收入与总消费之间的关系，并按照聚类结果进行区分，绘制散点图和小提琴图： #constrained_layout=True确保了图形的布局自动调整以避免重叠 fig, axd = plt.subplot_mosaic([[0,0],[1,2]], constrained_layout=True, figsize=(14,8)) fig.suptitle('Income vs Total Spent', weight='bold', fontsize=16) sns.scatterplot(data=df, x='Income', y='Total_Spent', hue='Cluster', palette='Blues', ax=axd[0]) sns.violinplot(data=df, x='Cluster', y='Total_Spent', palette='Blues', ax=axd[1]) sns.violinplot(data=df, x='Cluster', y='Income', palette='Blues', ax=axd[2]) 从图中可知： 类0：高消费 \u0026 高收入 类1：低消费 \u0026 低收入 类2：平均消费 \u0026 平均收入 类3：消费最低 \u0026 收入最低 类4：消费最高 \u0026 收入最高 ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:6:0","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"小提琴图怎么看？ 查看宽度：小提琴图的形状可以揭示数据分布的特征。例如，如果小提琴图在某一端特别宽，则表明在那个数值附近的数据点更加密集。 分析箱线图：内嵌的箱线图提供了数据的五数概括，帮助我们了解数据的分布范围和中位数的位置。 绘制“年龄”和“总花费”的核密度估计图（KDE)： g = sns.FacetGrid(data=df, col='Cluster') g.map(sns.kdeplot, 'Age', 'Total_Spent', color='#95C8D8', fill=True) ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:6:1","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"核密度估计图怎么看 核密度估计图（KDE图），是一种用于展示变量分布的图形，它可以看作是直方图的平滑版本。通过KDE图，我们可以直观地了解数据的分布情况，包括数据的集中趋势、分散程度和偏态情况。以下是如何解读核密度估计图的几个要点： 峰值（Peaks）：KDE图上的峰值表示数据中的众数区域，即该值附近的数据点较多。一个图上可以有多个峰值，表明数据分布可能是多模的（即存在多个众数）。 宽度（Width）：图形的宽度反映了数据的变异性或分散程度。宽度较大的核密度图表示数据点分布较为分散；相反，宽度较窄的图表示数据点比较集中。 偏态（Skewness）：如果KDE图不是完全对称的，那么数据分布可能是偏斜的。如果图形向右延伸更长，说明数据呈正偏态（右偏），即较多的数据值位于平均值的左侧；如果图形向左延伸更长，则数据呈负偏态（左偏），即较多的数据值位于平均值的右侧。 尾部（Tails）：KDE图的尾部可以告诉我们数据分布的尾部行为，例如是否存在长尾或者极端值。长尾指的是图形的一端延伸得很长，表明存在一些极端的高值或低值。 叠加图：有时候，研究者会在同一张图上绘制多个KDE图以比较不同子集的数据分布。这时，你可以通过观察哪些区域重叠较多，以及各个图形的形状差异，来了解不同子集之间的分布差异。 核密度估计与实际数据：需要注意的是，核密度估计图表示的是对数据分布的估计，而不是实际的数据分布。因此，它对于揭示数据的整体趋势很有帮助，但可能不适合用于精确的统计测试。 绘制“是否为父母”和“总花费”的核密度估计图： g = sns.FacetGrid(data=df, col='Cluster') g.map(sns.kdeplot, 'Is_Parent', 'Total_Spent', color='#95C8D8', fill=True) plt.text(0.6,3900, '0: Non Parent\\n1: Parent', weight='bold', fontsize=12) 绘制“儿童数量”和“总花费”的核密度估计图： g = sns.FacetGrid(data=df, col='Cluster') g.map(sns.kdeplot, 'Kidhome', 'Total_Spent', color='#95C8D8', fill=True) 绘制“青少年数量”和“总花费”的核密度估计图： g = sns.FacetGrid(data=df, col='Cluster') g.map(sns.kdeplot, 'Teenhome', 'Total_Spent', color='#95C8D8', fill=True) 绘制“孩子总量”和“总花费”的核密度估计图： g = sns.FacetGrid(data=df, col='Cluster') g.map(sns.kdeplot, 'Total_Children', 'Total_Spent', color='#95C8D8', fill=True) 绘制“婚恋状况”和“总花费”的核密度估计图： g = sns.FacetGrid(data=df, col='Cluster') g.map(sns.kdeplot, 'Living_With', 'Total_Spent', color='#95C8D8', fill=True) plt.text(0.7,3900, '0: Alone\\n1: Partner', weight='bold', fontsize=12) 绘制“学历情况”和“总花费”的散点图： g = sns.FacetGrid(data=df, col='Cluster') g.map(sns.scatterplot, 'Education', 'Total_Spent', color='#95C8D8') plt.text(0.6,3400, '0: Graduate\\n1: Postgraduate\\n2: Undergraduate', weight='bold', fontsize=12) ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:6:2","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["Python"],"content":"散点图怎么看？ 相关性（Correlation） 正相关：如果一个变量的增加伴随着另一个变量的增加，那么这两个变量之间存在正相关。在散点图中，这表现为点集总体上从左下到右上的趋势。 负相关：如果一个变量的增加伴随着另一个变量的减少，那么这两个变量之间存在负相关。在散点图中，这表现为点集总体上从左上到右下的趋势。 无相关：如果两个变量之间没有明显的线性关系，点集在图上分布较为均匀，没有明显的趋势。 集群（Clusters） 散点图上的点有时会形成一个或多个集群，表明数据中存在不同的子群。这些集群可能代表了不同的类别、组或具有相似特征的数据点。 异常值（Outliers） 在散点图中，远离其他数据点的点称为异常值。这些点可能表示了数据录入错误、测量误差或者实际的数据分布特性。 趋势线（Trend Line） 有时在散点图上会添加一条趋势线（例如线性回归线），以更清楚地显示变量之间的主要趋势。趋势线的斜率和方向提供了变量之间关系的直观表示。 分布密度 在某些散点图中，点的密集程度可以反映数据在该区域的集中性。点越密集的区域表明那里的数据点越多。 绘制“顾客类别”与“顾客最近一次消费时间”之间的条形图： plt.figure(figsize=(9,4)) plt.title(\"Average Number of Days Since Customer's Last Purchase\\nby Clusters\", weight='bold', fontsize=16) sns.barplot(data=df, x='Cluster', y='Recency', palette='Blues', ci=None) 观察不同顾客群体（Cluster）在不同产品类别（如葡萄酒、水果、肉类等）中的平均消费额： fig, axes = plt.subplots(2,3, figsize=(16,8)) fig.suptitle(\"Customer's Average Spent on Products\\nby Clusters\", weight='bold', fontsize=20) axes = axes.flatten() sns.barplot(data=df, x='Cluster', y='Wines', palette='Blues', ci=None, ax=axes[0]) axes[0].set_title('Wines', weight='bold') sns.barplot(data=df, x='Cluster', y='Fruits', palette='Blues', ci=None, ax=axes[1]) axes[1].set_title('Fruits', weight='bold') sns.barplot(data=df, x='Cluster', y='Meats', palette='Blues', ci=None, ax=axes[2]) axes[2].set_title('Meats', weight='bold') sns.barplot(data=df, x='Cluster', y='Fish', palette='Blues', ci=None, ax=axes[3]) axes[3].set_title('Fish', weight='bold') sns.barplot(data=df, x='Cluster', y='Sweets', palette='Blues', ci=None, ax=axes[4]) axes[4].set_title('Sweets', weight='bold') sns.barplot(data=df, x='Cluster', y='Golds', palette='Blues', ci=None, ax=axes[5]) axes[5].set_title('Gold', weight='bold') #调整子图的布局，确保子图之间有足够的空间，标题和坐标轴标签不会重叠 plt.tight_layout() 观察不同顾客群体在不同销售渠道中的平均消费次数： fig, axes = plt.subplots(1,3, figsize=(16,5)) fig.suptitle(\"Average Number of Purchases Made\\nThrough Different Methods by Clusters\", weight='bold', fontsize=16) axes = axes.flatten() custom_ylim = (0, 8) plt.setp(axes[0], ylim=custom_ylim) plt.setp(axes[1], ylim=custom_ylim) sns.barplot(data=df, x='Cluster', y='Web', palette='Blues', ci=None, ax=axes[0]) axes[0].set_title('Web', weight='bold') sns.barplot(data=df, x='Cluster', y='Catalog', palette='Blues', ci=None, ax=axes[1]) axes[1].set_title('Catalog', weight='bold') sns.barplot(data=df, x='Cluster', y='Store', palette='Blues', ci=None, ax=axes[2]) axes[2].set_title('Store', weight='bold') plt.tight_layout() 观察不同顾客群体在不同营销campaign中的平均消费次数： fig, axes = plt.subplots(2,3, figsize=(16,8)) fig.suptitle(\"Average Number of Purchases Made\\nThrough Different Campaigns by Clusters\", weight='bold', fontsize=20) axes = axes.flatten() sns.barplot(data=df, x='Cluster', y='AcceptedCmp1', palette='Blues', ci=None, ax=axes[0]) axes[0].set_title('Campaign 1', weight='bold') sns.barplot(data=df, x='Cluster', y='AcceptedCmp2', palette='Blues', ci=None, ax=axes[1]) axes[1].set_title('Campaign 2', weight='bold') sns.barplot(data=df, x='Cluster', y='AcceptedCmp3', palette='Blues', ci=None, ax=axes[2]) axes[2].set_title('Campaign 3', weight='bold') sns.barplot(data=df, x='Cluster', y='AcceptedCmp4', palette='Blues', ci=None, ax=axes[3]) axes[3].set_title('Campaign 4', weight='bold') sns.barplot(data=df, x='Cluster', y='AcceptedCmp5', palette='Blues', ci=None, ax=axes[4]) axes[4].set_title('Campaign 5', weight='bold') sns.barplot(data=df, x='Cluster', y='AcceptedCmp6', palette='Blues', ci=None, ax=axes[5]) axes[5].set_title('Campaign 6', weight='bold') plt.tight_layout() 观察不同顾客群体使用折扣进行消费的情况： plt.figure(figsize=(9,4)) plt.title('Average Number of Purchases Made with a Discount\\nby Clusters', weight='bold', fontsize=16) sns.barplot(data=df, x='Cluster', y='NumDealsPurchases', ci=None, palette='Blues') 综合以上多个图表，我们可以总结出以下5类用户画像： 类0： 高消费 \u0026 高收入 跨越各个年龄段 大部分未成为父母 成为父母的，家里有1个青少年 类1： 低消费 \u0026 低收入 跨越各个年龄段 大部分已成为父母 最多3个小孩 类2： 平均消费 \u0026 平均收入 相对更年长 几乎都已成为父母，除了一小部分人 最多3个小孩，主要是青少年 热衷于用折扣进行购买 类3： 消费最低 \u0026 收入最低 相对更年轻 最多2个小孩，主要是儿童 类4： 消费最高 \u0026 收入最高 跨越各个年龄段 大部分未成为父母 6次营销campaign都积极参加 ","date":"2024-04-07","objectID":"/zh-cn/posts/customer-profiling-and-segmentation/:6:3","tags":["Python"],"title":"用Python进行顾客用户分析和细分（含源码）","uri":"/zh-cn/posts/customer-profiling-and-segmentation/"},{"categories":["SEM"],"content":"背景介绍 毕业之后我误打误撞进入了一家互联网公司负责海外推广，一年后转行做海外广告优化师，从此开启我的谷歌广告之路。推过新品、负责过高净额的产品，也经历过整个账户被封，最近开始学习招聘得力的下属。虽然这几年市场情况不乐观，但就我个人体验而言，从薪资和市场需求度的角度出发，海外广告优化师还算是一个不错的职业选择。这篇文章希望能为那些对谷歌广告优化师这个岗位感兴趣的人提供一些参考。 ","date":"2023-01-26","objectID":"/zh-cn/posts/what-does-a-google-ads-specialist-do/:1:0","tags":["SEM","谷歌广告"],"title":"谷歌广告优化师是做什么的？","uri":"/zh-cn/posts/what-does-a-google-ads-specialist-do/"},{"categories":["SEM"],"content":"概念辨析 谷歌广告优化师对英语水平要求高吗？ 在大部分情况下，是的。 一般设有海外广告优化师岗位的公司都有出海的需求，所以外语水平是一定会考核的能力。通常要求至少是英语六级水平（如果你有相关经验，可能可以放宽到英语四级水平），部分要求比较高的公司会询问你具体的六级分数，如果是刚过及格线，也有可能被看作减分项。对于小语种专业的同学，小语种会是你的加分项，但是英语水平也必须要过关。 有一种例外情况可能不会限制求职者的英语水平：该公司的目标市场人群使用的是简体中文或繁体中文。这类产品的广告投放区域大多为中国大陆、香港、澳门、台湾，部分也可能面向其它国家地区的华人群体，比如新加坡、马来西亚等。 谷歌广告优化师会很忙吗？需要经常加班吗？ 如果有同学大概了解过海外广告优化师这个岗位，可能会看到有不少从业者都在劝退，理由是“加班非常严重”、“没有个人时间”、“节假日都需要盯着账户”等等。必须承认的是，加班对于这个岗位而言是比较普遍的情况，但是不能一概而论，根据我自身以及周围相关从业者的情况，是否加班以及加班强度会受到以下两个因素的影响： 甲乙方公司 海外广告投放也是有甲乙方公司的区分的。 甲方公司指的是公司有自研的产品（游戏、软件、实体产品等），然后聘用专门人员负责这些产品的海外广告。乙方公司指的是为甲方公司提供广告管理或咨询服务的公司，业内通常称作代理商（广告代理商），规模比较大的有：木瓜移动、飞书深诺、蓝色光标等。 绝大多数情况下，乙方工作人员的加班情况会严重得多。因为甲方人员顶上天也就负责整个公司所有产品的广告账户，而乙方工作人员的手上通常至少有五家客户的广告账户，每家客户需要推广的产品数量不等。我了解到比较吓人的数字是，有位之前在百度做乙方的优化师的手里有一百多个账户。 所以，很显然，能找到甲方工作就不要去乙方公司。 在筛选岗位和公司的时候，其实很容易能够看出这个岗位是属于甲方还是乙方。以下面这个招聘信息为例： 在招聘信息当中出现类似”广告客户“、“广告主”的字眼，基本就是乙方公司的岗位了。 推广产品类型 比较常见的产品类型有：游戏、软件、实体产品（如服装、鞋袜、饰品等）。对于软件和实体产品，还会存在C端和B端的区分。一般而言，加班强度如下（先看toB还是toC，再看产品类型）： C端 \u003e B端 游戏 \u003e 实体产品（偏电商）\u003e 软件（非游戏类） 广告流量的大小和变化幅度、市场竞争程度决定了你的加班频率和强度。 另外顺便提一句，谷歌广告优化师的加班强度通常没有Facebook广告优化师的加班强度高，因为两个平台的算法机制不同，后者对数据连续性的要求比较高，由于存在时差，通常需要优化师半夜爬起来看账户是否异常。 谷歌广告优化师只负责谷歌渠道吗？相关的经验可以用到其它平台上吗？ 谷歌广告优化师通常只负责谷歌渠道，部分规模较小的公司里可能会出现一名优化师同时负责多个渠道（例如Facebook、Tiktok、Instagram等）的情况。 关于经验是否可以复用，需要根据不同平台投放和算法机制来判断。如果直接对标的话，Google广告投放的经验可以直接复用在Bing广告，但是放到Facebook广告是肯定行不通的。 ","date":"2023-01-26","objectID":"/zh-cn/posts/what-does-a-google-ads-specialist-do/:2:0","tags":["SEM","谷歌广告"],"title":"谷歌广告优化师是做什么的？","uri":"/zh-cn/posts/what-does-a-google-ads-specialist-do/"},{"categories":["SEM"],"content":"工作内容 谷歌广告优化师的日常工作包括以下几块： 监控账户流量、费用、销售等的变化情况，及时根据数据波动进行调整，确保账户稳定性 关注市场中竞品的情况，发掘新的流量和销售拓展点，建立新的投放计划 与产品、运营、设计等其他部门对接（例如广告素材需求的对接、落地页设计、网站页面故障排查等） 日常的数据统计和分析工作（日报、周报、月报等） ","date":"2023-01-26","objectID":"/zh-cn/posts/what-does-a-google-ads-specialist-do/:3:0","tags":["SEM","谷歌广告"],"title":"谷歌广告优化师是做什么的？","uri":"/zh-cn/posts/what-does-a-google-ads-specialist-do/"},{"categories":["SEM"],"content":"任职要求 Google广告优化师的任职条件其实在各大招聘软件上都可以找到，我这里大概列几点行业内的通用要求： 英语水平 前面已经提到过英语能力对于这个岗位的重要性和必要性。目前而言，英语仍然是全球使用最广泛的语言，小语种的广告只能针对特定的小语种地区进行投放。英语专业的同学最好能提供专业八级的证书。如果没有考到，也可以另外考下托福或者雅思，作为英语水平的补充证明（非英专同学如果没有拿到四级或六级证书也是一样的准备思路）。 数据分析能力 这里的数据分析并不是指SQL、建模之类比较高阶的分析能力，而是要求你对广告数据有足够的敏感度，能够通过分析广告的相关指标做出比较合理的决策和操作。账户中各个数据指标的定义不同，但彼此之前都有一定的关联，你如何找到波动的原因，如何将你的分析讲清楚，如何向你的上级解释账户数据的异常以及对应的解决方案，都要求你有一定的数据分析思维和能力。 跨团队协作能力 广告投放不是单打独斗就能做好的岗位。优化师需要对自己的广告账户数据负责，经过分析和排查后，如果异常不是来自于广告本身，而是产品出现了bug、网页无法访问等等，优化师就需要找到对应的负责人去排查和解决。“社恐”只会让自己陷入被动。 ","date":"2023-01-26","objectID":"/zh-cn/posts/what-does-a-google-ads-specialist-do/:4:0","tags":["SEM","谷歌广告"],"title":"谷歌广告优化师是做什么的？","uri":"/zh-cn/posts/what-does-a-google-ads-specialist-do/"},{"categories":["SEM"],"content":"结语 以上就是关于谷歌广告优化师这一岗位的小小分享，后续可能会再讲讲简历以及面试如何准备。 ","date":"2023-01-26","objectID":"/zh-cn/posts/what-does-a-google-ads-specialist-do/:5:0","tags":["SEM","谷歌广告"],"title":"谷歌广告优化师是做什么的？","uri":"/zh-cn/posts/what-does-a-google-ads-specialist-do/"}]